Steps,Policy/Entropy,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Curiosity Value Estimate,Environment/Cumulative Reward,Policy/Extrinsic Reward,Policy/Curiosity Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Losses/Curiosity Forward Loss,Losses/Curiosity Inverse Loss,Is Training
20000,2.8036554,403.44444444444446,0.23785941,0.028008478,-1.271551107863585,-1.271551107863585,3.4735266528195803,0.050375663,0.022539701,0.0002996903,3.5291646,3.1255348,1.0
40000,2.7896204,486.5897435897436,0.076026194,0.3374741,-1.155064107133792,-1.155064107133792,5.2801470984107794,0.018104263,0.019789807,0.00029922568,0.60300577,3.0191517,1.0
60000,2.7657378,525.7567567567568,0.004495258,0.48600766,-1.1358729697562553,-1.1358729697562553,3.657499121250333,0.007168016,0.024287263,0.00029860655,0.32264346,2.9866462,1.0
80000,2.7650654,770.2222222222222,-0.042002413,0.5125049,-1.1619851818239246,-1.1619851818239246,4.10350836371934,0.004422948,0.023409048,0.00029798746,0.24956003,2.916239,1.0
100000,2.7422318,672.7586206896551,-0.06349762,0.49941158,-1.0717931032694619,-1.0717931032694619,2.8537650311301492,0.002723116,0.026634324,0.00029736856,0.2188198,2.8521492,1.0
120000,2.7449808,820.84,-0.07579332,0.4735742,-1.143907979130745,-1.143907979130745,2.8962354265898465,0.0033235152,0.024508862,0.0002967495,0.17309824,2.816678,1.0
140000,2.7489522,577.7222222222222,-0.10353803,0.43042943,-1.0659638949566417,-1.0659638949566417,1.759593474549345,0.0026961882,0.01785208,0.0002961331,0.14556924,2.8056426,1.0
160000,2.7381363,770.7391304347826,-0.108217806,0.3922227,-1.1214260918938594,-1.1214260918938594,2.106564972711646,0.00325226,0.027680269,0.0002955146,0.13810605,2.7674594,1.0
180000,2.7384772,793.7407407407408,-0.12254943,0.35243705,-0.9633999800792447,-0.9633999800792447,2.126458350405166,0.0010757472,0.027152298,0.0002948977,0.1255627,2.779536,1.0
